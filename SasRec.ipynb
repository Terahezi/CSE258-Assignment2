{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Terahezi/CSE258-Assignment2/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1Chl2UrBf16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/CSE258 Assignment2/sasRec')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwQkU11DCGx4",
        "colab_type": "code",
        "outputId": "b7f9bac0-5a40-4393-96d3-f3963ae60405",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbpBq6VbQX6E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def positional_encoding(dim, sentence_length, dtype=tf.float32):\n",
        "\n",
        "    encoded_vec = np.array([pos/np.power(10000, 2*i/dim) for pos in range(sentence_length) for i in range(dim)])\n",
        "    encoded_vec[::2] = np.sin(encoded_vec[::2])\n",
        "    encoded_vec[1::2] = np.cos(encoded_vec[1::2])\n",
        "\n",
        "    return tf.convert_to_tensor(encoded_vec.reshape([sentence_length, dim]), dtype=dtype)\n",
        "\n",
        "def normalize(inputs, \n",
        "              epsilon = 1e-8,\n",
        "              scope=\"ln\",\n",
        "              reuse=None):\n",
        "    '''Applies layer normalization.\n",
        "    \n",
        "    Args:\n",
        "      inputs: A tensor with 2 or more dimensions, where the first dimension has\n",
        "        `batch_size`.\n",
        "      epsilon: A floating number. A very small number for preventing ZeroDivision Error.\n",
        "      scope: Optional scope for `variable_scope`.\n",
        "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
        "        by the same name.\n",
        "      \n",
        "    Returns:\n",
        "      A tensor with the same shape and data dtype as `inputs`.\n",
        "    '''\n",
        "    with tf.variable_scope(scope, reuse=reuse):\n",
        "        inputs_shape = inputs.get_shape()\n",
        "        params_shape = inputs_shape[-1:]\n",
        "    \n",
        "        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
        "        beta= tf.Variable(tf.zeros(params_shape))\n",
        "        gamma = tf.Variable(tf.ones(params_shape))\n",
        "        normalized = (inputs - mean) / ( (variance + epsilon) ** (.5) )\n",
        "        outputs = gamma * normalized + beta\n",
        "        \n",
        "    return outputs\n",
        "\n",
        "def embedding(inputs, \n",
        "              vocab_size, \n",
        "              num_units, \n",
        "              zero_pad=True, \n",
        "              scale=True,\n",
        "              l2_reg=0.0,\n",
        "              scope=\"embedding\", \n",
        "              with_t=False,\n",
        "              reuse=None):\n",
        "    '''Embeds a given tensor.\n",
        "\n",
        "    Args:\n",
        "      inputs: A `Tensor` with type `int32` or `int64` containing the ids\n",
        "         to be looked up in `lookup table`.\n",
        "      vocab_size: An int. Vocabulary size.\n",
        "      num_units: An int. Number of embedding hidden units.\n",
        "      zero_pad: A boolean. If True, all the values of the fist row (id 0)\n",
        "        should be constant zeros.\n",
        "      scale: A boolean. If True. the outputs is multiplied by sqrt num_units.\n",
        "      scope: Optional scope for `variable_scope`.\n",
        "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
        "        by the same name.\n",
        "\n",
        "    Returns:\n",
        "      A `Tensor` with one more rank than inputs's. The last dimensionality\n",
        "        should be `num_units`.\n",
        "        \n",
        "    For example,\n",
        "    \n",
        "    ```\n",
        "    import tensorflow as tf\n",
        "    \n",
        "    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))\n",
        "    outputs = embedding(inputs, 6, 2, zero_pad=True)\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        print sess.run(outputs)\n",
        "    >>\n",
        "    [[[ 0.          0.        ]\n",
        "      [ 0.09754146  0.67385566]\n",
        "      [ 0.37864095 -0.35689294]]\n",
        "\n",
        "     [[-1.01329422 -1.09939694]\n",
        "      [ 0.7521342   0.38203377]\n",
        "      [-0.04973143 -0.06210355]]]\n",
        "    ```\n",
        "    \n",
        "    ```\n",
        "    import tensorflow as tf\n",
        "    \n",
        "    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))\n",
        "    outputs = embedding(inputs, 6, 2, zero_pad=False)\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        print sess.run(outputs)\n",
        "    >>\n",
        "    [[[-0.19172323 -0.39159766]\n",
        "      [-0.43212751 -0.66207761]\n",
        "      [ 1.03452027 -0.26704335]]\n",
        "\n",
        "     [[-0.11634696 -0.35983452]\n",
        "      [ 0.50208133  0.53509563]\n",
        "      [ 1.22204471 -0.96587461]]]    \n",
        "    ```    \n",
        "    '''\n",
        "    with tf.variable_scope(scope, reuse=reuse):\n",
        "        lookup_table = tf.get_variable('lookup_table',\n",
        "                                       dtype=tf.float32,\n",
        "                                       shape=[vocab_size, num_units],\n",
        "                                       #initializer=tf.contrib.layers.xavier_initializer(),\n",
        "                                       regularizer=tf.contrib.layers.l2_regularizer(l2_reg))\n",
        "        if zero_pad:\n",
        "            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),\n",
        "                                      lookup_table[1:, :]), 0)\n",
        "        outputs = tf.nn.embedding_lookup(lookup_table, inputs)\n",
        "        \n",
        "        if scale:\n",
        "            outputs = outputs * (num_units ** 0.5) \n",
        "    if with_t: return outputs,lookup_table\n",
        "    else: return outputs\n",
        "\n",
        "\n",
        "def multihead_attention(queries, \n",
        "                        keys, \n",
        "                        num_units=None, \n",
        "                        num_heads=8, \n",
        "                        dropout_rate=0,\n",
        "                        is_training=True,\n",
        "                        causality=False,\n",
        "                        scope=\"multihead_attention\", \n",
        "                        reuse=None,\n",
        "                        with_qk=False):\n",
        "    '''Applies multihead attention.\n",
        "    \n",
        "    Args:\n",
        "      queries: A 3d tensor with shape of [N, T_q, C_q].\n",
        "      keys: A 3d tensor with shape of [N, T_k, C_k].\n",
        "      num_units: A scalar. Attention size.\n",
        "      dropout_rate: A floating point number.\n",
        "      is_training: Boolean. Controller of mechanism for dropout.\n",
        "      causality: Boolean. If true, units that reference the future are masked. \n",
        "      num_heads: An int. Number of heads.\n",
        "      scope: Optional scope for `variable_scope`.\n",
        "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
        "        by the same name.\n",
        "        \n",
        "    Returns\n",
        "      A 3d tensor with shape of (N, T_q, C)  \n",
        "    '''\n",
        "    with tf.variable_scope(scope, reuse=reuse):\n",
        "        # Set the fall back option for num_units\n",
        "        if num_units is None:\n",
        "            num_units = queries.get_shape().as_list[-1]\n",
        "        \n",
        "        # Linear projections\n",
        "        # Q = tf.layers.dense(queries, num_units, activation=tf.nn.relu) # (N, T_q, C)\n",
        "        # K = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)\n",
        "        # V = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)\n",
        "        Q = tf.layers.dense(queries, num_units, activation=None) # (N, T_q, C)\n",
        "        K = tf.layers.dense(keys, num_units, activation=None) # (N, T_k, C)\n",
        "        V = tf.layers.dense(keys, num_units, activation=None) # (N, T_k, C)\n",
        "        \n",
        "        # Split and concat\n",
        "        Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0) # (h*N, T_q, C/h) \n",
        "        K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0) # (h*N, T_k, C/h) \n",
        "        V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0) # (h*N, T_k, C/h) \n",
        "\n",
        "        # Multiplication\n",
        "        outputs = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1])) # (h*N, T_q, T_k)\n",
        "        \n",
        "        # Scale\n",
        "        outputs = outputs / (K_.get_shape().as_list()[-1] ** 0.5)\n",
        "        \n",
        "        # Key Masking\n",
        "        key_masks = tf.sign(tf.abs(tf.reduce_sum(keys, axis=-1))) # (N, T_k)\n",
        "        key_masks = tf.tile(key_masks, [num_heads, 1]) # (h*N, T_k)\n",
        "        key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(queries)[1], 1]) # (h*N, T_q, T_k)\n",
        "        \n",
        "        paddings = tf.ones_like(outputs)*(-2**32+1)\n",
        "        outputs = tf.where(tf.equal(key_masks, 0), paddings, outputs) # (h*N, T_q, T_k)\n",
        "  \n",
        "        # Causality = Future blinding\n",
        "        if causality:\n",
        "            diag_vals = tf.ones_like(outputs[0, :, :]) # (T_q, T_k)\n",
        "            tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense() # (T_q, T_k)\n",
        "            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1]) # (h*N, T_q, T_k)\n",
        "   \n",
        "            paddings = tf.ones_like(masks)*(-2**32+1)\n",
        "            outputs = tf.where(tf.equal(masks, 0), paddings, outputs) # (h*N, T_q, T_k)\n",
        "  \n",
        "        # Activation\n",
        "        outputs = tf.nn.softmax(outputs) # (h*N, T_q, T_k)\n",
        "         \n",
        "        # Query Masking\n",
        "        query_masks = tf.sign(tf.abs(tf.reduce_sum(queries, axis=-1))) # (N, T_q)\n",
        "        query_masks = tf.tile(query_masks, [num_heads, 1]) # (h*N, T_q)\n",
        "        query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(keys)[1]]) # (h*N, T_q, T_k)\n",
        "        outputs *= query_masks # broadcasting. (N, T_q, C)\n",
        "          \n",
        "        # Dropouts\n",
        "        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))\n",
        "               \n",
        "        # Weighted sum\n",
        "        outputs = tf.matmul(outputs, V_) # ( h*N, T_q, C/h)\n",
        "        \n",
        "        # Restore shape\n",
        "        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2 ) # (N, T_q, C)\n",
        "              \n",
        "        # Residual connection\n",
        "        outputs += queries\n",
        "              \n",
        "        # Normalize\n",
        "        #outputs = normalize(outputs) # (N, T_q, C)\n",
        " \n",
        "    if with_qk: return Q,K\n",
        "    else: return outputs\n",
        "\n",
        "def feedforward(inputs, \n",
        "                num_units=[2048, 512],\n",
        "                scope=\"multihead_attention\", \n",
        "                dropout_rate=0.2,\n",
        "                is_training=True,\n",
        "                reuse=None):\n",
        "    '''Point-wise feed forward net.\n",
        "    \n",
        "    Args:\n",
        "      inputs: A 3d tensor with shape of [N, T, C].\n",
        "      num_units: A list of two integers.\n",
        "      scope: Optional scope for `variable_scope`.\n",
        "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
        "        by the same name.\n",
        "        \n",
        "    Returns:\n",
        "      A 3d tensor with the same shape and dtype as inputs\n",
        "    '''\n",
        "    with tf.variable_scope(scope, reuse=reuse):\n",
        "        # Inner layer\n",
        "        params = {\"inputs\": inputs, \"filters\": num_units[0], \"kernel_size\": 1,\n",
        "                  \"activation\": tf.nn.relu, \"use_bias\": True}\n",
        "        outputs = tf.layers.conv1d(**params)\n",
        "        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))\n",
        "        # Readout layer\n",
        "        params = {\"inputs\": outputs, \"filters\": num_units[1], \"kernel_size\": 1,\n",
        "                  \"activation\": None, \"use_bias\": True}\n",
        "        outputs = tf.layers.conv1d(**params)\n",
        "        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))\n",
        "        \n",
        "        # Residual connection\n",
        "        outputs += inputs\n",
        "        \n",
        "        # Normalize\n",
        "        #outputs = normalize(outputs)\n",
        "    \n",
        "    return outputs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yhc-VgDaQOVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model():\n",
        "    def __init__(self, usernum, itemnum, args, reuse=None):\n",
        "        self.is_training = tf.placeholder(tf.bool, shape=())\n",
        "        self.u = tf.placeholder(tf.int32, shape=(None))\n",
        "        self.input_seq = tf.placeholder(tf.int32, shape=(None, args['maxlen']))\n",
        "        self.pos = tf.placeholder(tf.int32, shape=(None, args['maxlen']))\n",
        "        self.neg = tf.placeholder(tf.int32, shape=(None, args['maxlen']))\n",
        "        pos = self.pos\n",
        "        neg = self.neg\n",
        "        mask = tf.expand_dims(tf.to_float(tf.not_equal(self.input_seq, 0)), -1)\n",
        "\n",
        "        with tf.variable_scope(\"SASRec\", reuse=reuse):\n",
        "            # sequence embedding, item embedding table\n",
        "            self.seq, item_emb_table = embedding(self.input_seq,\n",
        "                                                 vocab_size=itemnum + 1,\n",
        "                                                 num_units=args['hidden_units'],\n",
        "                                                 zero_pad=True,\n",
        "                                                 scale=True,\n",
        "                                                 l2_reg=args['l2_emb'],\n",
        "                                                 scope=\"input_embeddings\",\n",
        "                                                 with_t=True,\n",
        "                                                 reuse=reuse\n",
        "                                                 )\n",
        "\n",
        "            # Positional Encoding\n",
        "            t, pos_emb_table = embedding(\n",
        "                tf.tile(tf.expand_dims(tf.range(tf.shape(self.input_seq)[1]), 0), [tf.shape(self.input_seq)[0], 1]),\n",
        "                vocab_size=args['maxlen'],\n",
        "                num_units=args['hidden_units'],\n",
        "                zero_pad=False,\n",
        "                scale=False,\n",
        "                l2_reg=args['l2_emb'],\n",
        "                scope=\"dec_pos\",\n",
        "                reuse=reuse,\n",
        "                with_t=True\n",
        "            )\n",
        "            self.seq += t\n",
        "\n",
        "            # Dropout\n",
        "            self.seq = tf.layers.dropout(self.seq,\n",
        "                                         rate=args['dropout_rate'],\n",
        "                                         training=tf.convert_to_tensor(self.is_training))\n",
        "            self.seq *= mask\n",
        "\n",
        "            # Build blocks\n",
        "\n",
        "            for i in range(args['num_blocks']):\n",
        "                with tf.variable_scope(\"num_blocks_%d\" % i):\n",
        "\n",
        "                    # Self-attention\n",
        "                    self.seq = multihead_attention(queries=normalize(self.seq),\n",
        "                                                   keys=self.seq,\n",
        "                                                   num_units=args['hidden_units'],\n",
        "                                                   num_heads=args['num_heads'],\n",
        "                                                   dropout_rate=args['dropout_rate'],\n",
        "                                                   is_training=self.is_training,\n",
        "                                                   causality=True,\n",
        "                                                   scope=\"self_attention\")\n",
        "\n",
        "                    # Feed forward\n",
        "                    self.seq = feedforward(normalize(self.seq), num_units=[args['hidden_units'], args['hidden_units']],\n",
        "                                           dropout_rate=args['dropout_rate'], is_training=self.is_training)\n",
        "                    self.seq *= mask\n",
        "\n",
        "            self.seq = normalize(self.seq)\n",
        "\n",
        "        pos = tf.reshape(pos, [tf.shape(self.input_seq)[0] * args['maxlen']])\n",
        "        neg = tf.reshape(neg, [tf.shape(self.input_seq)[0] * args['maxlen']])\n",
        "        pos_emb = tf.nn.embedding_lookup(item_emb_table, pos)\n",
        "        neg_emb = tf.nn.embedding_lookup(item_emb_table, neg)\n",
        "        seq_emb = tf.reshape(self.seq, [tf.shape(self.input_seq)[0] * args['maxlen'], args['hidden_units']])\n",
        "\n",
        "        self.test_item = tf.placeholder(tf.int32, shape=(101))\n",
        "        test_item_emb = tf.nn.embedding_lookup(item_emb_table, self.test_item)\n",
        "        self.test_logits = tf.matmul(seq_emb, tf.transpose(test_item_emb))\n",
        "        self.test_logits = tf.reshape(self.test_logits, [tf.shape(self.input_seq)[0], args['maxlen'], 101])\n",
        "        self.test_logits = self.test_logits[:, -1, :]\n",
        "\n",
        "        # prediction layer\n",
        "        self.pos_logits = tf.reduce_sum(pos_emb * seq_emb, -1)\n",
        "        self.neg_logits = tf.reduce_sum(neg_emb * seq_emb, -1)\n",
        "\n",
        "        # ignore padding items (0)\n",
        "        istarget = tf.reshape(tf.to_float(tf.not_equal(pos, 0)), [tf.shape(self.input_seq)[0] * args['maxlen']])\n",
        "        self.loss = tf.reduce_sum(\n",
        "            - tf.log(tf.sigmoid(self.pos_logits) + 1e-24) * istarget -\n",
        "            tf.log(1 - tf.sigmoid(self.neg_logits) + 1e-24) * istarget\n",
        "        ) / tf.reduce_sum(istarget)\n",
        "        reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
        "        self.loss += sum(reg_losses)\n",
        "\n",
        "        tf.summary.scalar('loss', self.loss)\n",
        "        self.auc = tf.reduce_sum(\n",
        "            ((tf.sign(self.pos_logits - self.neg_logits) + 1) / 2) * istarget\n",
        "        ) / tf.reduce_sum(istarget)\n",
        "\n",
        "        if reuse is None:\n",
        "            tf.summary.scalar('auc', self.auc)\n",
        "            self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "            self.optimizer = tf.train.AdamOptimizer(learning_rate=args['lr'], beta2=0.98)\n",
        "            self.train_op = self.optimizer.minimize(self.loss, global_step=self.global_step)\n",
        "        else:\n",
        "            tf.summary.scalar('test_auc', self.auc)\n",
        "\n",
        "        self.merged = tf.summary.merge_all()\n",
        "\n",
        "    def predict(self, sess, u, seq, item_idx):\n",
        "        return sess.run(self.test_logits,\n",
        "                        {self.u: u, self.input_seq: seq, self.test_item: item_idx, self.is_training: False})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdMZR93SQObq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from multiprocessing import Process, Queue\n",
        "\n",
        "\n",
        "def random_neq(l, r, s):\n",
        "    t = np.random.randint(l, r)\n",
        "    while t in s:\n",
        "        t = np.random.randint(l, r)\n",
        "    return t\n",
        "\n",
        "\n",
        "def sample_function(user_train, usernum, itemnum, batch_size, maxlen, result_queue, SEED):\n",
        "    def sample():\n",
        "\n",
        "        user = np.random.randint(1, usernum + 1)\n",
        "        while len(user_train[user]) <= 1: user = np.random.randint(1, usernum + 1)\n",
        "\n",
        "        seq = np.zeros([maxlen], dtype=np.int32)\n",
        "        pos = np.zeros([maxlen], dtype=np.int32)\n",
        "        neg = np.zeros([maxlen], dtype=np.int32)\n",
        "        nxt = user_train[user][-1]\n",
        "        idx = maxlen - 1\n",
        "\n",
        "        ts = set(user_train[user])\n",
        "        for i in reversed(user_train[user][:-1]):\n",
        "            seq[idx] = i\n",
        "            pos[idx] = nxt\n",
        "            if nxt != 0: neg[idx] = random_neq(1, itemnum + 1, ts)\n",
        "            nxt = i\n",
        "            idx -= 1\n",
        "            if idx == -1: break\n",
        "\n",
        "        return (user, seq, pos, neg)\n",
        "\n",
        "    np.random.seed(SEED)\n",
        "    while True:\n",
        "        one_batch = []\n",
        "        for i in range(batch_size):\n",
        "            one_batch.append(sample())\n",
        "\n",
        "        result_queue.put(zip(*one_batch))\n",
        "\n",
        "\n",
        "class WarpSampler(object):\n",
        "    def __init__(self, User, usernum, itemnum, batch_size=64, maxlen=10, n_workers=1):\n",
        "        self.result_queue = Queue(maxsize=n_workers * 10)\n",
        "        self.processors = []\n",
        "        for i in range(n_workers):\n",
        "            self.processors.append(\n",
        "                Process(target=sample_function, args=(User,\n",
        "                                                      usernum,\n",
        "                                                      itemnum,\n",
        "                                                      batch_size,\n",
        "                                                      maxlen,\n",
        "                                                      self.result_queue,\n",
        "                                                      np.random.randint(2e9)\n",
        "                                                      )))\n",
        "            self.processors[-1].daemon = True\n",
        "            self.processors[-1].start()\n",
        "\n",
        "    def next_batch(self):\n",
        "        return self.result_queue.get()\n",
        "\n",
        "    def close(self):\n",
        "        for p in self.processors:\n",
        "            p.terminate()\n",
        "            p.join()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVzq58ClQOeS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import copy\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def data_partition(fname):\n",
        "    usernum = 0\n",
        "    itemnum = 0\n",
        "    User = defaultdict(list)\n",
        "    user_train = {}\n",
        "    user_valid = {}\n",
        "    user_test = {}\n",
        "    # assume user/item index starting from 1\n",
        "    f = open('data/%s.txt' % fname, 'r')\n",
        "    for line in f:\n",
        "        u, i = line.rstrip().split(' ')\n",
        "        u = int(u)\n",
        "        i = int(i)\n",
        "        usernum = max(u, usernum)\n",
        "        itemnum = max(i, itemnum)\n",
        "        User[u].append(i)\n",
        "\n",
        "    for user in User:\n",
        "        nfeedback = len(User[user])\n",
        "        if nfeedback < 3:\n",
        "            user_train[user] = User[user]\n",
        "            user_valid[user] = []\n",
        "            user_test[user] = []\n",
        "        else:\n",
        "            user_train[user] = User[user][:-2]\n",
        "            user_valid[user] = []\n",
        "            user_valid[user].append(User[user][-2])\n",
        "            user_test[user] = []\n",
        "            user_test[user].append(User[user][-1])\n",
        "    return [user_train, user_valid, user_test, usernum, itemnum]\n",
        "\n",
        "\n",
        "def evaluate(model, dataset, args, sess):\n",
        "    [train, valid, test, usernum, itemnum] = copy.deepcopy(dataset)\n",
        "\n",
        "    NDCG = 0.0\n",
        "    HT = 0.0\n",
        "    valid_user = 0.0\n",
        "\n",
        "    if usernum>10000:\n",
        "        users = random.sample(range(1, usernum + 1), 10000)\n",
        "    else:\n",
        "        users = range(1, usernum + 1)\n",
        "    for u in users:\n",
        "\n",
        "        if len(train[u]) < 1 or len(test[u]) < 1: continue\n",
        "\n",
        "        seq = np.zeros([args['maxlen']], dtype=np.int32)\n",
        "        idx = args['maxlen'] - 1\n",
        "        seq[idx] = valid[u][0]\n",
        "        idx -= 1\n",
        "        for i in reversed(train[u]):\n",
        "            seq[idx] = i\n",
        "            idx -= 1\n",
        "            if idx == -1: break\n",
        "        rated = set(train[u])\n",
        "        rated.add(0)\n",
        "        item_idx = [test[u][0]]\n",
        "        for _ in range(100):\n",
        "            t = np.random.randint(1, itemnum + 1)\n",
        "            while t in rated: t = np.random.randint(1, itemnum + 1)\n",
        "            item_idx.append(t)\n",
        "\n",
        "        predictions = -model.predict(sess, [u], [seq], item_idx)\n",
        "        predictions = predictions[0]\n",
        "\n",
        "        rank = predictions.argsort().argsort()[0]\n",
        "\n",
        "        valid_user += 1\n",
        "\n",
        "        if rank < 10:\n",
        "            NDCG += 1 / np.log2(rank + 2)\n",
        "            HT += 1\n",
        "        if valid_user % 100 == 0:\n",
        "            print('.')\n",
        "            sys.stdout.flush()\n",
        "\n",
        "    return NDCG / valid_user, HT / valid_user\n",
        "\n",
        "\n",
        "def evaluate_valid(model, dataset, args, sess):\n",
        "    [train, valid, test, usernum, itemnum] = copy.deepcopy(dataset)\n",
        "\n",
        "    NDCG = 0.0\n",
        "    valid_user = 0.0\n",
        "    HT = 0.0\n",
        "    if usernum>10000:\n",
        "        users = random.sample(range(1, usernum + 1), 10000)\n",
        "    else:\n",
        "        users = range(1, usernum + 1)\n",
        "    for u in users:\n",
        "        if len(train[u]) < 1 or len(valid[u]) < 1: continue\n",
        "\n",
        "        seq = np.zeros([args['maxlen']], dtype=np.int32)\n",
        "        idx = args['maxlen'] - 1\n",
        "        for i in reversed(train[u]):\n",
        "            seq[idx] = i\n",
        "            idx -= 1\n",
        "            if idx == -1: break\n",
        "\n",
        "        rated = set(train[u])\n",
        "        rated.add(0)\n",
        "        item_idx = [valid[u][0]]\n",
        "        for _ in range(100):\n",
        "            t = np.random.randint(1, itemnum + 1)\n",
        "            while t in rated: t = np.random.randint(1, itemnum + 1)\n",
        "            item_idx.append(t)\n",
        "\n",
        "        predictions = -model.predict(sess, [u], [seq], item_idx)\n",
        "        predictions = predictions[0]\n",
        "\n",
        "        rank = predictions.argsort().argsort()[0]\n",
        "\n",
        "        valid_user += 1\n",
        "\n",
        "        if rank < 10:\n",
        "            NDCG += 1 / np.log2(rank + 2)\n",
        "            HT += 1\n",
        "        if valid_user % 100 == 0:\n",
        "            print('.')\n",
        "            #sys.stdout.flush()\n",
        "\n",
        "    return NDCG / valid_user, HT / valid_user\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6hXN35ZkEqk",
        "colab_type": "text"
      },
      "source": [
        "## Start training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v08s6_VMCIoY",
        "colab_type": "code",
        "outputId": "aba4325d-9b78-486f-e96b-45395cd3f596",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import time\n",
        "#import argparse\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def str2bool(s):\n",
        "    if s not in {'False', 'True'}:\n",
        "        raise ValueError('Not a valid boolean string')\n",
        "    return s == 'True'\n",
        "\n",
        "\n",
        "# parser = argparse.ArgumentParser()\n",
        "# parser.add_argument('--dataset', required=True)\n",
        "# parser.add_argument('--train_dir', required=True)\n",
        "# parser.add_argument('--batch_size', default=128, type=int)\n",
        "# parser.add_argument('--lr', default=0.001, type=float)\n",
        "# parser.add_argument('--maxlen', default=50, type=int)\n",
        "# parser.add_argument('--hidden_units', default=50, type=int)\n",
        "# parser.add_argument('--num_blocks', default=2, type=int)\n",
        "# parser.add_argument('--num_epochs', default=201, type=int)\n",
        "# parser.add_argument('--num_heads', default=1, type=int)\n",
        "# parser.add_argument('--dropout_rate', default=0.5, type=float)\n",
        "# parser.add_argument('--l2_emb', default=0.0, type=float)\n",
        "\n",
        "# args = parser.parse_args()\n",
        "\n",
        "# parameters\n",
        "args = dict()\n",
        "args['root'] = '/content/drive/My Drive/CSE258 Assignment2/'\n",
        "args['dataset_path'] = 'ml-1m'\n",
        "args['train_dir'] = 'default'\n",
        "args['batch_size'] = 128\n",
        "args['lr'] = 0.001\n",
        "args['maxlen'] = 50\n",
        "args['hidden_units'] = 50\n",
        "args['num_blocks'] = 2\n",
        "args['num_epochs'] = 201\n",
        "args['num_heads'] = 1\n",
        "args['dropout_rate'] = 0.5\n",
        "args['l2_emb'] = 0.0\n",
        "\n",
        "\n",
        "if not os.path.isdir(args['dataset_path'] + '_' + args['train_dir']):\n",
        "    os.makedirs(args['dataset_path'] + '_' + args['train_dir'])\n",
        "# with open(os.path.join(dataset_path + '_' + train_dir, 'args.txt'), 'w') as f:\n",
        "#     f.write('\\n'.join([str(k) + ',' + str(v) for k, v in sorted(vars(args).items(), key=lambda x: x[0])]))\n",
        "# f.close()\n",
        "\n",
        "dataset = data_partition(args['dataset_path'])\n",
        "[user_train, user_valid, user_test, usernum, itemnum] = dataset\n",
        "num_batch = len(user_train) // args['batch_size']\n",
        "cc = 0.0\n",
        "for u in user_train:\n",
        "    cc += len(user_train[u])\n",
        "print('average sequence length: %.2f' % (cc / len(user_train)))\n",
        "\n",
        "f = open(os.path.join(args['dataset_path'] + '_' + args['train_dir'], 'log.txt'), 'w')\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "config.allow_soft_placement = True\n",
        "sess = tf.Session(config=config)\n",
        "\n",
        "sampler = WarpSampler(user_train, usernum, itemnum, batch_size=args['batch_size'], maxlen=args['maxlen'], n_workers=3)\n",
        "model = Model(usernum, itemnum, args)\n",
        "sess.run(tf.initialize_all_variables())\n",
        "\n",
        "T = 0.0\n",
        "t0 = time.time()\n",
        "\n",
        "for epoch in range(1, args['num_epochs'] + 1):\n",
        "    print(epoch)\n",
        "    for step in tqdm(range(num_batch), total=num_batch, ncols=70, leave=False, unit='b'):\n",
        "        u, seq, pos, neg = sampler.next_batch()\n",
        "        auc, loss, _ = sess.run([model.auc, model.loss, model.train_op],\n",
        "                                {model.u: u, model.input_seq: seq, model.pos: pos, model.neg: neg,\n",
        "                                    model.is_training: True})\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        t1 = time.time() - t0\n",
        "        T += t1\n",
        "        print('Evaluating')\n",
        "        t_test = evaluate(model, dataset, args, sess)\n",
        "        t_valid = evaluate_valid(model, dataset, args, sess)\n",
        "        print('.')\n",
        "        print('epoch:%d, time: %f(s), valid (NDCG@10: %.4f, HR@10: %.4f), test (NDCG@10: %.4f, HR@10: %.4f)' % (\n",
        "        epoch, T, t_valid[0], t_valid[1], t_test[0], t_test[1]))\n",
        "\n",
        "        f.write(str(t_valid) + ' ' + str(t_test) + '\\n')\n",
        "        f.flush()\n",
        "        t0 = time.time()\n",
        "\n",
        "f.close()\n",
        "sampler.close()\n",
        "print(\"Done\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average sequence length: 163.50\n",
            "WARNING:tensorflow:From <ipython-input-5-f4b8b87a6257>:10: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "INFO:tensorflow:Scale of 0 disables regularizer.\n",
            "INFO:tensorflow:Scale of 0 disables regularizer.\n",
            "WARNING:tensorflow:From <ipython-input-5-f4b8b87a6257>:42: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:271: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-4-4b68babd3505>:161: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From <ipython-input-4-4b68babd3505>:182: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From <ipython-input-4-4b68babd3505>:242: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv1D` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/tf_should_use.py:198: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|                                           | 0/47 [00:00<?, ?b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  9%|██▉                                | 4/47 [00:00<00:01, 32.86b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  6%|██▏                                | 3/47 [00:00<00:01, 24.36b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  6%|██▏                                | 3/47 [00:00<00:02, 19.49b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  4%|█▍                                 | 2/47 [00:00<00:02, 16.00b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  6%|██▏                                | 3/47 [00:00<00:02, 21.17b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  4%|█▍                                 | 2/47 [00:00<00:02, 19.70b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  6%|██▏                                | 3/47 [00:00<00:02, 19.89b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  4%|█▍                                 | 2/47 [00:00<00:02, 18.30b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  4%|█▍                                 | 2/47 [00:00<00:02, 19.50b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  4%|█▍                                 | 2/47 [00:00<00:02, 17.35b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  4%|█▍                                 | 2/47 [00:00<00:02, 16.90b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  4%|█▍                                 | 2/47 [00:00<00:02, 16.07b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "13\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  6%|██▏                                | 3/47 [00:00<00:02, 20.80b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "14\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  4%|█▍                                 | 2/47 [00:00<00:02, 19.26b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  6%|██▏                                | 3/47 [00:00<00:02, 19.59b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  4%|█▍                                 | 2/47 [00:00<00:02, 17.83b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "17\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  6%|██▏                                | 3/47 [00:00<00:01, 25.60b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "18\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  6%|██▏                                | 3/47 [00:00<00:01, 23.66b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "19\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  6%|██▏                                | 3/47 [00:00<00:01, 24.61b/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluating\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-a1d92dc70340>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mT\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Evaluating'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mt_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mt_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-28ed20dcc9b5>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, dataset, args, sess)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0musers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musernum\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0musers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musernum\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m \u001b[0;32min\u001b[0m \u001b[0musers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'xrange' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfBIVbKeWjjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}